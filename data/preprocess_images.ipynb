{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hallando la cantidad de imágenes de cada tipo en cada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el directorio actual del notebook\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Directorio donde están las carpetas dataset1, dataset2, dataset3\n",
    "data_dir = notebook_dir\n",
    "\n",
    "# Lista de datasets (carpetas dataset1, dataset2, dataset3)\n",
    "datasets = ['dataset1', 'dataset2', 'dataset3']\n",
    "\n",
    "# Lista de categorías\n",
    "categories = ['Covid', 'Normal', 'Pneumonia']\n",
    "\n",
    "# Lista de fases de aprendizaje \n",
    "fases = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para contar imágenes en un directorio específico\n",
    "def count_images_in_directory(directory):\n",
    "    count = 0\n",
    "    if os.path.exists(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.jpeg') or filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dataset1\n",
      "Fase: train\n",
      "Número de imágenes en la categoría Covid: 111\n",
      "Número de imágenes en la categoría Normal: 70\n",
      "Número de imágenes en la categoría Pneumonia: 70\n",
      "Fase: test\n",
      "Número de imágenes en la categoría Covid: 26\n",
      "Número de imágenes en la categoría Normal: 20\n",
      "Número de imágenes en la categoría Pneumonia: 20\n",
      "\n",
      "Dataset: dataset2\n",
      "Fase: train\n",
      "Número de imágenes en la categoría Covid: 1626\n",
      "Número de imágenes en la categoría Normal: 1802\n",
      "Número de imágenes en la categoría Pneumonia: 1800\n",
      "Fase: test\n",
      "Número de imágenes en la categoría Covid: 0\n",
      "Número de imágenes en la categoría Normal: 0\n",
      "Número de imágenes en la categoría Pneumonia: 0\n",
      "\n",
      "Dataset: dataset3\n",
      "Fase: train\n",
      "Número de imágenes en la categoría Covid: 460\n",
      "Número de imágenes en la categoría Normal: 1266\n",
      "Número de imágenes en la categoría Pneumonia: 3418\n",
      "Fase: test\n",
      "Número de imágenes en la categoría Covid: 116\n",
      "Número de imágenes en la categoría Normal: 317\n",
      "Número de imágenes en la categoría Pneumonia: 855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contar imágenes en cada dataset y categoría\n",
    "n_images = []\n",
    "for dataset in datasets:\n",
    "    dataset_dir = os.path.join(data_dir, dataset)\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for fase in fases:\n",
    "        fase_dir = os.path.join(dataset_dir, fase)\n",
    "        print(f\"Fase: {fase}\")\n",
    "        for category in categories:\n",
    "            category_dir = os.path.join(fase_dir, category)\n",
    "            num_images = count_images_in_directory(category_dir)\n",
    "            print(f\"Número de imágenes en la categoría {category}: {num_images}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocesando las imágenes\n",
    "\n",
    "Se tomarán las imágenes y se las llevará a un un formato cuadrado de longitud tamaño `SIZE`. Así, todas las imágenes tendrán el mismo tamaño. Se las ubicará en un directorio de acuerdo a su categoría, sin importan si inicialmente venían cargadas como datos de train o test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:09<00:00, 11.48it/s]ata\\.\\dataset1:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 26.71it/s]\n",
      "100%|██████████| 70/70 [00:07<00:00,  8.91it/s]\\data\\.\\dataset1:  33%|███▎      | 1/3 [00:10<00:21, 10.66s/it]\n",
      "100%|██████████| 20/20 [00:01<00:00, 10.35it/s]\n",
      "100%|██████████| 70/70 [00:04<00:00, 14.02it/s]\\data\\.\\dataset1:  67%|██████▋   | 2/3 [00:20<00:10, 10.16s/it]\n",
      "100%|██████████| 20/20 [00:01<00:00, 18.82it/s]\n",
      "Procesando c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset1: 100%|██████████| 3/3 [00:26<00:00,  8.84s/it]\n",
      "100%|██████████| 1626/1626 [00:57<00:00, 28.41it/s]a\\.\\dataset2:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Procesando c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2:  33%|███▎      | 1/3 [00:57<01:54, 57.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2\\test\\Covid no existe. Saltando.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1802/1802 [01:01<00:00, 29.09it/s]\n",
      "Procesando c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2:  67%|██████▋   | 2/3 [01:59<01:00, 60.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2\\test\\Normal no existe. Saltando.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:59<00:00, 30.23it/s]\n",
      "Procesando c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2: 100%|██████████| 3/3 [02:59<00:00, 59.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El directorio c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset2\\test\\Pneumonia no existe. Saltando.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 460/460 [00:34<00:00, 13.49it/s]ata\\.\\dataset3:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 116/116 [00:09<00:00, 11.78it/s]\n",
      "100%|██████████| 1266/1266 [01:23<00:00, 15.09it/s]a\\.\\dataset3:  33%|███▎      | 1/3 [00:43<01:27, 43.99s/it]\n",
      "100%|██████████| 317/317 [00:21<00:00, 14.59it/s]\n",
      "100%|██████████| 3418/3418 [02:18<00:00, 24.61it/s]a\\.\\dataset3:  67%|██████▋   | 2/3 [02:29<01:20, 80.32s/it]\n",
      "100%|██████████| 855/855 [00:30<00:00, 28.14it/s]\n",
      "Procesando c:\\machine_learning\\MLF_FinalProject\\data\\.\\dataset3: 100%|██████████| 3/3 [05:19<00:00, 106.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Definir directorios de origen y destino\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(base_dir, '.')\n",
    "output_dir = os.path.join(base_dir, 'processed_data')\n",
    "categories = ['Covid', 'Normal', 'Pneumonia']\n",
    "target_size = (SIZE, SIZE)\n",
    "\n",
    "# Crear directorios de salida\n",
    "for category in categories:\n",
    "    os.makedirs(os.path.join(output_dir, category))\n",
    "\n",
    "def preprocess_and_save_images(source_dir, category, target_size):\n",
    "    dest_path = os.path.join(output_dir, category)\n",
    "    \n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"El directorio {source_dir} no existe. Saltando.\")\n",
    "        return\n",
    "    \n",
    "    for img_name in tqdm(os.listdir(source_dir)):\n",
    "        try:\n",
    "            img_path = os.path.join(source_dir, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Redimensionar y recortar la imagen\n",
    "            img = ImageOps.fit(img, target_size)\n",
    "            \n",
    "            img.save(os.path.join(dest_path, img_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {img_path}: {e}\")\n",
    "\n",
    "def process_dataset(dataset_dir, target_size):\n",
    "    for category in tqdm(categories, desc=f\"Procesando {dataset_dir}\"):\n",
    "        # Procesar imágenes de train si existen\n",
    "        train_dir = os.path.join(dataset_dir, 'train', category)\n",
    "        if os.path.exists(train_dir):\n",
    "            preprocess_and_save_images(train_dir, category, target_size)\n",
    "        else:\n",
    "            print(f\"El directorio {train_dir} no existe. Saltando.\")\n",
    "\n",
    "        # Procesar imágenes de test si existen\n",
    "        test_dir = os.path.join(dataset_dir, 'test', category)\n",
    "        if os.path.exists(test_dir):\n",
    "            preprocess_and_save_images(test_dir, category, target_size)\n",
    "        else:\n",
    "            print(f\"El directorio {test_dir} no existe. Saltando.\")\n",
    "\n",
    "# Procesar todos los datasets\n",
    "for dataset in ['dataset1', 'dataset2', 'dataset3']:\n",
    "    dataset_dir = os.path.join(data_dir, dataset)\n",
    "    process_dataset(dataset_dir, target_size)\n",
    "\n",
    "print(\"Preprocesamiento completo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos finalmente que todas las imágnes tienen el mismo tamaño, y, en el caso de que todas tengan el tamaño deseado, retornamos el número de imágenes preprocesadas de cada cateoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2334/2334 [00:01<00:00, 2223.27it/s]\n",
      "100%|██████████| 3475/3475 [00:02<00:00, 1201.13it/s]\n",
      "100%|██████████| 6163/6163 [00:47<00:00, 131.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las imágenes en c:\\machine_learning\\MLF_FinalProject\\data\\processed_data tienen el tamaño esperado.\n",
      "Número de imágenes en c:\\machine_learning\\MLF_FinalProject\\data\\processed_data: [2334, 3475, 6163]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar que todas las imágnes tienen el mismo tamaño\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "processed_data_dir = os.path.join(os.getcwd(), 'processed_data')\n",
    "categories = ['Covid', 'Normal', 'Pneumonia']\n",
    "numImages = [0, 0, 0]\n",
    "\n",
    "def check_image_sizes(dataset_dir):\n",
    "    for i, category in enumerate(categories):\n",
    "        category_dir = os.path.join(dataset_dir, category)\n",
    "        for img_name in tqdm(os.listdir(category_dir)):\n",
    "            img_path = os.path.join(category_dir, img_name)\n",
    "            img = Image.open(img_path)\n",
    "            if img.size != (SIZE, SIZE):\n",
    "                print(f\"La imagen {img_path} tiene un tamaño diferente al esperado.\")\n",
    "                return\n",
    "            else:\n",
    "                numImages[i] += 1\n",
    "\n",
    "    print(f\"Todas las imágenes en {dataset_dir} tienen el tamaño esperado.\")\n",
    "    print(f\"Número de imágenes en {dataset_dir}: {numImages}\")\n",
    "    return\n",
    "\n",
    "check_image_sizes(processed_data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
